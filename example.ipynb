{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage for `mix_gamma_vi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mix_gamma_vi import mix_gamma_vi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10000 data from a mixture of gamma two gamma distributions. Called this tensor `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "pi_true = [0.5, 0.5]\n",
    "a_true  = [20,  80 ]\n",
    "B_true  = [20,  40 ]\n",
    "\n",
    "mix_gamma = tfp.distributions.MixtureSameFamily(\n",
    "    mixture_distribution=tfp.distributions.Categorical(probs=pi_true),\n",
    "    components_distribution=tfp.distributions.Gamma(concentration=a_true, rate=B_true))\n",
    "\n",
    "x = mix_gamma.sample(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference Under the Shape-Mean Parameterisation (Recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defualt parameterisation for the function `mix_gamma_vi` is the mean-shape parameterisation under which the variational approximations to the posterior are\n",
    "\n",
    "\\begin{align*}\n",
    "q^*(\\mathbf{\\pi}) &= \\mathrm{Dirichlet} \\left( \\zeta_1, ..., \\zeta_K \\right) ,  \\\\\n",
    "q^*(\\alpha_k) &= \\mathcal{N}(\\hat{\\alpha}_k, \\sigma_j^2) ,  \\\\\n",
    "q^* (\\mu_k) &=  \\operatorname{Inv-Gamma} \\left( \\gamma_k, \\lambda_k \\right) .  \n",
    "\\end{align*}\n",
    "\n",
    "The product approximates the joint posterior\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{\\pi}, \\mathbf{\\alpha}, \\mathbf{\\mu} \\mid \\mathbf{x}) &= q^*(\\mathbf{\\pi}) \\prod_{k=1}^K q^*(\\alpha_k) q^*(\\mu_k).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pi': <tf.Tensor: id=2891, shape=(1, 2), dtype=float32, numpy=array([[0.5057488, 0.4942512]], dtype=float32)>,\n",
       " 'mu': <tf.Tensor: id=2898, shape=(1, 2), dtype=float32, numpy=array([[1.0018914, 1.9988744]], dtype=float32)>,\n",
       " 'alpha': <tf.Tensor: id=2902, shape=(1, 2), dtype=float32, numpy=array([[20.098001, 79.798294]], dtype=float32)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a model\n",
    "fit = mix_gamma_vi(x, 2)\n",
    "\n",
    "# Get the fitted distribution\n",
    "distribution = fit.distribution()\n",
    "\n",
    "# Get the means of the parameters under the fitted posterior\n",
    "distribution.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pi': <tf.Tensor: id=2912, shape=(1, 2), dtype=float32, numpy=array([[0.00499892, 0.00499892]], dtype=float32)>,\n",
       " 'mu': <tf.Tensor: id=2925, shape=(1, 2), dtype=float32, numpy=array([[0.00314254, 0.00318285]], dtype=float32)>,\n",
       " 'alpha': <tf.Tensor: id=2929, shape=(1, 2), dtype=float32, numpy=array([[0.3996685, 1.6052226]], dtype=float32)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the posterior standard deviations\n",
    "distribution.stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference Under the Shape-Rate Parameterisation (Not Recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional parameterisation for gamma distribution is the shape-rate parameterisation which this package also supports (although it is not recommended). In this case, the variational approximations to the posterior are\n",
    "\n",
    "\\begin{align*}\n",
    "q^*(\\mathbf{\\pi}) &= \\mathrm{Dirichlet} \\left( \\zeta_1, ..., \\zeta_K \\right) ,  \\\\\n",
    "q^*(\\alpha_k) &= \\mathcal{N}(\\hat{\\alpha}_k, \\sigma_k^2) , \\\\\n",
    "q^* (\\beta_k) &=  \\operatorname{Gamma} \\left( \\gamma_j, \\lambda_j \\right) .  \n",
    "\\end{align*}\n",
    "\n",
    "The product approximates the joint posterior\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{\\pi}, \\mathbf{\\alpha}, \\mathbf{\\beta} \\mid \\mathbf{x}) &= q^*(\\mathbf{\\pi}) \\prod_{k=1}^K q^*(\\alpha_k) q^*(\\beta_k) .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pi': <tf.Tensor: id=2940, shape=(1, 2), dtype=float64, numpy=array([[0.50572743, 0.49427257]])>,\n",
       " 'beta': <tf.Tensor: id=2947, shape=(1, 2), dtype=float64, numpy=array([[0.05012314, 0.02520653]])>,\n",
       " 'alpha': <tf.Tensor: id=2951, shape=(1, 2), dtype=float64, numpy=array([[19.99132836, 79.29992946]])>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a model\n",
    "fit = mix_gamma_vi(x, 2, parameterisation=\"shape-rate\")\n",
    "\n",
    "# Get the fitted distribution\n",
    "distribution = fit.distribution()\n",
    "\n",
    "# Get the means of the parameters under the fitted posterior\n",
    "distribution.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pi': <tf.Tensor: id=2961, shape=(1, 2), dtype=float64, numpy=array([[0.00499892, 0.00499892]])>,\n",
       " 'beta': <tf.Tensor: id=2974, shape=(1, 2), dtype=float64, numpy=array([[1.57648090e-04, 4.02626111e-05]])>,\n",
       " 'alpha': <tf.Tensor: id=2978, shape=(1, 2), dtype=float64, numpy=array([[0.06243915, 0.12553762]])>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the posterior standard deviations\n",
    "distribution.stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the standard deviation of $\\mathbf{\\alpha}$ under the shape-rate parameterisation is much lower than it is under the shape-mean parameterisation. It turns out that the shape-mean parameterisation produces a posterior approximation that is much closer to that of a Gibbs sampler (the baseline) than that of the shape-rate parameterisation. For this reason, we recommend using the shape-mean parameterisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
